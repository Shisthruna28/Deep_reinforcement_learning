{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "gym.logger.set_level(40) \n",
    "import numpy as np\n",
    "import copy\n",
    "from collections import namedtuple, deque\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "import random\n",
    "from torch import multiprocessing\n",
    "torch.manual_seed(121) \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state space 3 \n",
      "action space 1\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Pendulum-v0').unwrapped\n",
    "env.seed(121)\n",
    "n_state=env.observation_space.shape[0]\n",
    "n_actions=env.action_space.shape[0]\n",
    "print(f'state space {n_state} \\naction space {n_actions}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition', ('state', 'action', 'mask', 'next_state',\n",
    "                                      'reward'))\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self):\n",
    "        self.memory = []\n",
    "\n",
    "    def push(self, *args):\n",
    "        self.memory.append(Transition(*args))\n",
    "    def sample(self, batch_size=None):\n",
    "        if batch_size is None:\n",
    "            return Transition(*zip(*self.memory))\n",
    "        else:\n",
    "            random_batch = random.sample(self.memory, batch_size)\n",
    "            return Transition(*zip(*random_batch))\n",
    "    def append(self, new_memory):\n",
    "        self.memory += new_memory.memory\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.9144734]\n",
      "[1.1966343]\n",
      "[-0.15408255]\n",
      "[1.1221167]\n",
      "[-1.5269023]\n",
      "[0.5596841]\n",
      "[-1.4265869]\n",
      "[1.7786757]\n",
      "[0.08739328]\n",
      "[-0.34135225]\n",
      "Episode 0 - total rewards obtained -90.62275300130331\n"
     ]
    }
   ],
   "source": [
    "# test buffer\n",
    "\"\"\"Random Agent\"\"\"\n",
    "Num_episode=1\n",
    "Time_step=10\n",
    "def run_episodes(env):\n",
    "    for episode in range(Num_episode):\n",
    "        env.reset()\n",
    "        returns=0\n",
    "        for t in range(Time_step):\n",
    "            action=env.action_space.sample()\n",
    "            print(action)\n",
    "            next_state,reward,done,_=env.step(action) \n",
    "            returns+=reward\n",
    "            if done:\n",
    "                print(f\"Episode {episode} finished after {t} timesteps\")\n",
    "                break\n",
    "        print(f\"Episode {episode} - total rewards obtained {returns}\")\n",
    "      \n",
    "run_episodes(env)   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"ppo_adv.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"ppo.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_entropy(std):\n",
    "    var = std.pow(2)\n",
    "    entropy = 0.5 + 0.5 * torch.log(2 * var * np.pi)\n",
    "    return entropy.sum(1, keepdim=True)\n",
    "def normal_log_density(x, mean, log_std):\n",
    "    std = torch.exp(log_std)\n",
    "    var = std.pow(2)\n",
    "    log_density = - torch.pow(x - mean, 2) / (2 * var) - 0.5 * np.log(2 * np.pi) - log_std\n",
    "    return log_density.sum(1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):    \n",
    "    def __init__(self, s_dim, a_dim):\n",
    "        super(Policy, self).__init__()\n",
    "        self.net = nn.Sequential(nn.Linear(s_dim, 64),\n",
    "                     nn.ReLU(),\n",
    "                     nn.Linear(64, 64),\n",
    "                     nn.ReLU(),\n",
    "                     nn.Linear(64, a_dim))\n",
    "        self.a_log_std = nn.Parameter(torch.zeros(1, a_dim))\n",
    "\n",
    "    def forward(self, s):\n",
    "        a_mean = self.net(s)\n",
    "        a_log_std = self.a_log_std.expand_as(a_mean)\n",
    "        return a_mean, a_log_std\n",
    "\n",
    "    def select_action(self, s):\n",
    "        a_mean, a_log_std = self.forward(s)\n",
    "        a = torch.normal(a_mean, torch.exp(a_log_std))\n",
    "        return a\n",
    "\n",
    "    def get_log_prob(self, s, a):\n",
    "        a_mean, a_log_std = self.forward(s)\n",
    "        log_prob = normal_log_density(a, a_mean, a_log_std)\n",
    "        return log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value(nn.Module):\n",
    "    def __init__(self, s_dim):\n",
    "        super(Value, self).__init__()\n",
    "        self.net = nn.Sequential(nn.Linear(s_dim, 64),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Linear(64, 64),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Linear(64, 1))\n",
    "\n",
    "    def forward(self, s):\n",
    "        value = self.net(s)\n",
    "        return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampler(pid, queue, env, policy, batchsz):\n",
    "    buff = ReplayMemory()\n",
    "    sampled_num = 0\n",
    "    sampled_traj_num = 0\n",
    "    traj_len = 200\n",
    "    real_traj_len = 0\n",
    "    avg_reward = []\n",
    "\n",
    "    while sampled_num < batchsz:\n",
    "        traj_reward = 0\n",
    "        s = env.reset()\n",
    "        for t in range(traj_len):\n",
    "            a = policy.select_action(Variable(torch.Tensor(s).unsqueeze(0))).data[0].numpy()\n",
    "            next_s, reward, done, _ = env.step(a)\n",
    "            mask = 0 if done else 1\n",
    "            buff.push(s, a, mask, next_s, reward)\n",
    "            s = next_s\n",
    "            traj_reward += reward\n",
    "            real_traj_len = t\n",
    "            if done:\n",
    "                break\n",
    "        sampled_num += real_traj_len\n",
    "        sampled_traj_num += 1\n",
    "        avg_reward.append(traj_reward)\n",
    "    avg_reward = sum(avg_reward) / len(avg_reward)\n",
    "    queue.put([pid, buff, avg_reward])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO:\n",
    "    gamma = 0.99\n",
    "    l2_reg = 0\n",
    "    lr = 3e-4\n",
    "    epsilon = 0.2\n",
    "    tau = 0.95\n",
    "    def __init__(self, env_cls, thread_num):\n",
    "        self.thread_num = thread_num\n",
    "        self.env_cls = env_cls\n",
    "        dummy_env = env_cls()\n",
    "        self.s_dim = dummy_env.observation_space.shape[0]\n",
    "        is_discrete_action = len(dummy_env.action_space.shape)\n",
    "        if is_discrete_action == 0:\n",
    "            self.a_dim = dummy_env.action_space.n\n",
    "            self.is_discrete_action = True\n",
    "        else:\n",
    "            self.a_dim = dummy_env.action_space.shape[0]\n",
    "            self.is_discrete_action = False\n",
    "        self.env_list = []\n",
    "        for _ in range(thread_num):\n",
    "            self.env_list.append(env_cls())\n",
    "        self.policy = Policy(self.s_dim, self.a_dim)\n",
    "        self.value = Value(self.s_dim)\n",
    "\n",
    "        self.policy_optim = optim.Adam(self.policy.parameters(), lr=self.lr)\n",
    "        self.value_optim = optim.Adam(self.value.parameters(), lr=self.lr)\n",
    "            \n",
    "    def est_adv(self, r, v, mask):\n",
    "        batch = v.size(0)\n",
    "        v_target = torch.Tensor(batch)\n",
    "        delta = torch.Tensor(batch)\n",
    "        A_sa = torch.Tensor(batch)\n",
    "        prev_v_target = 0\n",
    "        prev_v = 0\n",
    "        prev_A_sa = 0\n",
    "        for t in reversed(range(batch)):\n",
    "            v_target[t] = r[t] + self.gamma * prev_v_target * mask[t]\n",
    "            delta[t] = r[t] + self.gamma * prev_v * mask[t] - v[t]\n",
    "            A_sa[t] = delta[t] + self.gamma * self.tau * prev_A_sa * mask[t]\n",
    "            prev_v_target = v_target[t]\n",
    "            prev_v = v[t]\n",
    "            prev_A_sa = A_sa[t]\n",
    "        A_sa = (A_sa - A_sa.mean()) / A_sa.std()\n",
    "        return A_sa, v_target\n",
    "    \n",
    "    def update(self, batchsz):\n",
    "        batch = self.sample(batchsz)\n",
    "        s = torch.from_numpy(np.stack(batch.state))\n",
    "        a = torch.from_numpy(np.stack(batch.action))\n",
    "        r = torch.Tensor(np.stack(batch.reward))\n",
    "        mask = torch.Tensor(np.stack(batch.mask))\n",
    "        batchsz = s.size(0)\n",
    "        v = self.value(Variable(s)).data.squeeze()\n",
    "        log_pi_old_sa = self.policy.get_log_prob(Variable(s), Variable(a)).data\n",
    "        A_sa, v_target = self.est_adv(r, v, mask)\n",
    "        v_target = Variable(v_target)\n",
    "        A_sa = Variable(A_sa)\n",
    "        s = Variable(s)\n",
    "        a = Variable(a)\n",
    "        log_pi_old_sa = Variable(log_pi_old_sa)\n",
    "\n",
    "        for _ in range(5):\n",
    "\n",
    "            # 4.1 shuffle current batch\n",
    "            perm = torch.randperm(batchsz)\n",
    "            # shuffle the variable for mutliple optimize\n",
    "            v_target_shuf, A_sa_shuf, s_shuf, a_shuf, log_pi_old_sa_shuf = v_target[perm], A_sa[perm], s[perm], a[perm], \\\n",
    "                                                                           log_pi_old_sa[perm]\n",
    "\n",
    "            # 4.2 get mini-batch for optimizing\n",
    "            optim_batchsz = 4096\n",
    "            optim_chunk_num = int(np.ceil(batchsz / optim_batchsz))\n",
    "            # chunk the optim_batch for total batch\n",
    "            v_target_shuf, A_sa_shuf, s_shuf, a_shuf, log_pi_old_sa_shuf = torch.chunk(v_target_shuf, optim_chunk_num), \\\n",
    "                                                                           torch.chunk(A_sa_shuf, optim_chunk_num), \\\n",
    "                                                                           torch.chunk(s_shuf, optim_chunk_num), \\\n",
    "                                                                           torch.chunk(a_shuf, optim_chunk_num), \\\n",
    "                                                                           torch.chunk(log_pi_old_sa_shuf,\n",
    "                                                                                       optim_chunk_num)\n",
    "            # 4.3 iterate all mini-batch to optimize\n",
    "            for v_target_b, A_sa_b, s_b, a_b, log_pi_old_sa_b in zip(v_target_shuf, A_sa_shuf, s_shuf, a_shuf,\n",
    "                                                                     log_pi_old_sa_shuf):\n",
    "                # print('optim:', batchsz, v_target_b.size(), A_sa_b.size(), s_b.size(), a_b.size(), log_pi_old_sa_b.size())\n",
    "                # 1. update value network\n",
    "                v_b = self.value(s_b)\n",
    "                loss = torch.pow(v_b - v_target_b, 2).mean()\n",
    "                self.value_optim.zero_grad()\n",
    "                loss.backward()\n",
    "                # nn.utils.clip_grad_norm(self.value.parameters(), 4)\n",
    "                self.value_optim.step()\n",
    "\n",
    "                # 2. update policy network by clipping\n",
    "                # [b, 1]\n",
    "                log_pi_sa = self.policy.get_log_prob(s_b, a_b)\n",
    "                # ratio = exp(log_Pi(a|s) - log_Pi_old(a|s)) = Pi(a|s) / Pi_old(a|s)\n",
    "                # we use log_pi for stability of numerical operation\n",
    "                # [b, 1] => [b]\n",
    "                ratio = torch.exp(log_pi_sa - log_pi_old_sa_b).squeeze(1)\n",
    "                surrogate1 = ratio * A_sa_b\n",
    "                surrogate2 = torch.clamp(ratio, 1 - self.epsilon, 1 + self.epsilon) * A_sa_b\n",
    "                # this is element-wise comparing.\n",
    "                # we add negative symbol to convert gradient ascent to gradient descent.\n",
    "                surrogate = - torch.min(surrogate1, surrogate2).mean()\n",
    "                self.policy_optim.zero_grad()\n",
    "                surrogate.backward(retain_graph=True)\n",
    "                torch.nn.utils.clip_grad_norm_(self.policy.parameters(), 10)\n",
    "                self.policy_optim.step()\n",
    "   \n",
    "    def sample(self, batchsz):\n",
    "    \n",
    "        thread_batchsz = np.ceil(batchsz / self.thread_num).astype(np.int32)\n",
    "        queue = multiprocessing.Queue()\n",
    "        evt = multiprocessing.Event()\n",
    "        threads = []\n",
    "        for i in range(self.thread_num):\n",
    "            thread_args = (i, queue, self.env_list[i], self.policy, thread_batchsz)\n",
    "            threads.append(multiprocessing.Process(target=sampler, args=thread_args))\n",
    "        for t in threads:\n",
    "            t.daemon = True\n",
    "            t.start()\n",
    "\n",
    "        # we need to get the first ReplayMemory object and then merge others ReplayMemory use its append function.\n",
    "        pid0, buff0, avg_reward0 = queue.get()\n",
    "        avg_reward = [avg_reward0]\n",
    "        for _ in range(1, self.thread_num):\n",
    "            pid, buff_, avg_reward_ = queue.get()\n",
    "            buff0.append(buff_) # merge current ReplayMemory into buff0\n",
    "            avg_reward.append(avg_reward_)\n",
    "\n",
    "        # now buff saves all the sampled data and avg_reward is the average reward of current sampled data\n",
    "        buff = buff0\n",
    "        avg_reward = np.array(avg_reward).mean()\n",
    "\n",
    "        print('avg reward:', avg_reward)\n",
    "        return buff.sample()\n",
    "    \n",
    "    def render(self, interval=8):\n",
    "        thread = multiprocessing.Process(target=self.render_, args=(interval,))\n",
    "        thread.start()\n",
    "        \n",
    "    def render_(self, interval):\n",
    "        env = self.env_cls()\n",
    "        s = env.reset()\n",
    "\n",
    "        while True:\n",
    "            s = Variable(torch.Tensor(s)).unsqueeze(0)\n",
    "            a = self.policy.select_action(s).squeeze().data.numpy()\n",
    "            print(a)\n",
    "            s, r, done, _ = env.step(a)\n",
    "\n",
    "            env.render()\n",
    "\n",
    "            if done:\n",
    "                s = env.reset()\n",
    "                time.sleep(interval)\n",
    "    \n",
    "    def save(self, filename='ppo'):\n",
    "\n",
    "        torch.save(self.value.state_dict(), filename + '.val.mdl')\n",
    "        torch.save(self.policy.state_dict(), filename + '.pol.mdl')\n",
    "\n",
    "        print('saved network to mdl')\n",
    "\n",
    "    def load(self, filename='ppo'):\n",
    "        value_mdl = filename + '.val.mdl'\n",
    "        policy_mdl = filename + '.pol.mdl'\n",
    "        if os.path.exists(value_mdl):\n",
    "            self.value.load_state_dict(torch.load(value_mdl))\n",
    "            print('loaded checkpoint from file:', value_mdl)\n",
    "        if os.path.exists(policy_mdl):\n",
    "            self.policy.load_state_dict(torch.load(policy_mdl))\n",
    "        print('loaded checkpoint from file:', policy_mdl)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env():\n",
    "    env=gym.make('Pendulum-v0').unwrapped\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded checkpoint from file: ppo.val.mdl\n",
      "loaded checkpoint from file: ppo.pol.mdl\n",
      "3.7529173492021135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-818:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/satty/anaconda3/envs/rl/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/satty/anaconda3/envs/rl/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-25-1f1e7293597b>\", line 224, in render_\n",
      "    s, r, done, _ = env.step(a)\n",
      "  File \"/home/satty/anaconda3/envs/rl/lib/python3.6/site-packages/gym/envs/classic_control/pendulum.py\", line 37, in step\n",
      "    u = np.clip(u, -self.max_torque, self.max_torque)[0]\n",
      "IndexError: invalid index to scalar variable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg reward: -1364.7605578718028\n",
      "avg reward: -1392.1997531627449\n",
      "avg reward: -1393.683563335906\n",
      "avg reward: -1385.2519553157417\n",
      "avg reward: -1406.2934864401461\n",
      "avg reward: -1369.020550786967\n",
      "avg reward: -1381.8208466611973\n",
      "avg reward: -1371.4209307721264\n",
      "avg reward: -1379.8438413363497\n",
      "avg reward: -1390.3604794734376\n",
      "avg reward: -1373.026315030896\n",
      "avg reward: -1396.386007607114\n",
      "avg reward: -1385.6732747503597\n",
      "avg reward: -1343.2822575281998\n",
      "avg reward: -1376.2344226038972\n",
      "avg reward: -1368.644922168544\n",
      "avg reward: -1374.6841486120827\n",
      "avg reward: -1352.856089768198\n",
      "avg reward: -1361.8026275794382\n",
      "avg reward: -1342.0493130179846\n",
      "avg reward: -1346.5701871863605\n",
      "avg reward: -1356.2482045708932\n",
      "avg reward: -1320.4126012996699\n",
      "avg reward: -1316.4213216208157\n",
      "avg reward: -1345.9538290502567\n",
      "avg reward: -1342.1614412239637\n",
      "avg reward: -1329.529651367656\n",
      "avg reward: -1377.7027715109139\n",
      "avg reward: -1363.0959499602857\n",
      "avg reward: -1377.7075998102011\n",
      "avg reward: -1346.9099247144422\n",
      "avg reward: -1355.2088324213255\n",
      "avg reward: -1369.6152462694477\n",
      "avg reward: -1381.2574815023895\n",
      "avg reward: -1351.797883654574\n",
      "avg reward: -1326.4309670063424\n",
      "avg reward: -1321.0944566320209\n",
      "avg reward: -1352.3069266401499\n",
      "avg reward: -1350.1550803179869\n",
      "avg reward: -1357.4311015363717\n",
      "avg reward: -1344.7717096177923\n",
      "avg reward: -1348.766482155691\n",
      "avg reward: -1345.503335531606\n",
      "avg reward: -1322.550476347804\n",
      "avg reward: -1332.7822909158076\n",
      "avg reward: -1263.6491266683056\n",
      "avg reward: -1251.7293599808731\n",
      "avg reward: -1259.847043214651\n",
      "avg reward: -1307.387455361362\n",
      "avg reward: -1298.0140907409625\n",
      "avg reward: -1296.5672151745869\n",
      "avg reward: -1248.8538741085933\n",
      "avg reward: -1259.2469495698015\n",
      "avg reward: -1283.4555754817686\n",
      "avg reward: -1286.5355126121804\n",
      "avg reward: -1301.9978589842142\n",
      "avg reward: -1316.1790053077495\n",
      "avg reward: -1275.2715734036226\n",
      "avg reward: -1235.8079770652182\n",
      "avg reward: -1311.6583291591705\n",
      "avg reward: -1252.9898548067276\n",
      "avg reward: -1332.4924962924565\n",
      "avg reward: -1260.1137916976377\n",
      "avg reward: -1328.1589521433978\n",
      "avg reward: -1344.887350675044\n",
      "avg reward: -1318.0878676090347\n",
      "avg reward: -1328.183581297967\n",
      "avg reward: -1337.9835669103602\n",
      "avg reward: -1246.181913296877\n",
      "avg reward: -1272.4414143441631\n",
      "avg reward: -1218.9376936488295\n",
      "avg reward: -1253.708822689152\n",
      "avg reward: -1222.6970710160763\n",
      "avg reward: -1233.2075136818405\n",
      "avg reward: -1050.5667217047605\n",
      "avg reward: -1138.7579466818788\n",
      "avg reward: -1082.8860319560035\n",
      "avg reward: -1144.1254772751013\n",
      "avg reward: -1048.6057972005035\n",
      "avg reward: -1076.7378893382422\n",
      "avg reward: -1138.4308736980024\n",
      "avg reward: -1053.2082255127962\n",
      "avg reward: -1090.926054002363\n",
      "avg reward: -1065.6604840940495\n",
      "avg reward: -1028.7608914422449\n",
      "avg reward: -1028.2797436523892\n",
      "avg reward: -952.6738582878818\n",
      "avg reward: -958.4214281091853\n",
      "avg reward: -953.1562645363115\n",
      "avg reward: -977.1503448470279\n",
      "avg reward: -962.9531165222613\n",
      "avg reward: -990.5995440705543\n",
      "avg reward: -961.6425899329231\n",
      "avg reward: -952.4088000747905\n",
      "avg reward: -969.0457923383841\n",
      "avg reward: -1077.6889076367413\n",
      "avg reward: -958.7892572822602\n",
      "avg reward: -949.1021910175305\n",
      "avg reward: -1007.4892327791861\n",
      "avg reward: -1080.8067523969883\n",
      "avg reward: -1049.4746765012253\n",
      "saved network to mdl\n",
      "avg reward: -965.8142729565025\n",
      "avg reward: -963.8040026627621\n",
      "avg reward: -956.4104187319107\n",
      "avg reward: -1006.6269656827\n",
      "avg reward: -949.4986239663315\n",
      "avg reward: -950.4762519008694\n",
      "avg reward: -949.7135480319881\n",
      "avg reward: -940.4607660175582\n",
      "avg reward: -963.3337037147396\n",
      "avg reward: -918.6755938447617\n",
      "avg reward: -938.8430090259889\n",
      "avg reward: -951.3364141559722\n",
      "avg reward: -969.3584112732719\n",
      "avg reward: -956.6632272012639\n",
      "avg reward: -951.111466629533\n",
      "avg reward: -945.7482778869095\n",
      "avg reward: -952.3757178370214\n",
      "avg reward: -953.9405613048447\n",
      "avg reward: -949.822295622042\n",
      "avg reward: -944.8081462590136\n",
      "avg reward: -944.7785812764952\n",
      "avg reward: -948.7497880412743\n",
      "avg reward: -922.0009580392565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-943:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/satty/anaconda3/envs/rl/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/satty/anaconda3/envs/rl/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-7c115bf28e7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-28-7c115bf28e7e>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mppo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatchsz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-1f1e7293597b>\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, batchsz)\u001b[0m\n\u001b[1;32m     78\u001b[0m         \"\"\"\n\u001b[1;32m     79\u001b[0m         \u001b[0;31m# 1. sample data asynchronously\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatchsz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;31m# data in batch is : batch.state: ([1, s_dim], [1, s_dim]...)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-1f1e7293597b>\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, batchsz)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;31m# we need to get the first ReplayMemory object and then merge others ReplayMemory use its append function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mpid0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuff0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavg_reward0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m         \u001b[0mavg_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mavg_reward0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthread_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rl/lib/python3.6/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rlock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rl/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36mrecv_bytes\u001b[0;34m(self, maxlength)\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmaxlength\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmaxlength\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"negative maxlength\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbuf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bad_message_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rl/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_recv_bytes\u001b[0;34m(self, maxsize)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m         \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstruct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"!i\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmaxsize\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmaxsize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rl/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_recv\u001b[0;34m(self, size, read)\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0mremaining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m             \u001b[0mchunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"<ipython-input-24-05ac8c38086d>\", line 25, in sampler\n",
      "    while sampled_num < batchsz:\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    torch.set_default_tensor_type('torch.DoubleTensor')\n",
    "\n",
    "    batchsz = 32\n",
    "    ppo = PPO(make_env, 1)\n",
    "\n",
    "    # load model from checkpoint\n",
    "    ppo.load()\n",
    "    # comment this line to close evaluaton thread, to speed up training process.\n",
    "    ppo.render(1)\n",
    "\n",
    "    for i in range(10000):\n",
    "\n",
    "        ppo.update(batchsz)\n",
    "\n",
    "        if i % 100 == 0 and i:\n",
    "            ppo.save()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference:\n",
    "- https://github.com/dragen1860/PPO-Pytorch\n",
    "- https://github.com/ikostrikov/pytorch-a2c-ppo-acktr\n",
    "- https://github.com/dai-dao/PPO-Pytorch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
